# pr2drag/trackers/cotracker_v2.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Optional, Tuple
import numpy as np

from .base import TrackerOutput


def _require_torch() -> "tuple[Any, Any]":
    try:
        import torch  # type: ignore
        return torch, torch.cuda
    except Exception as e:
        raise ImportError(
            "[cotracker_v2] Missing dependency: torch. "
            "Install torch first in your environment."
        ) from e


def _pick_device(torch_mod, device: Optional[str]) -> str:
    if device is not None and device.strip():
        return device.strip()
    if torch_mod.cuda.is_available():
        return "cuda"
    return "cpu"


def _to_torch_video(torch_mod, video_uint8: np.ndarray, device: str):
    """
    video_uint8: [T,H,W,3] uint8
    returns: torch float32 [1,T,3,H,W] in [0,1]
    """
    if not isinstance(video_uint8, np.ndarray) or video_uint8.dtype != np.uint8:
        raise TypeError(f"[cotracker_v2] video_uint8 must be uint8 ndarray, got {type(video_uint8)} {getattr(video_uint8,'dtype',None)}")
    if video_uint8.ndim != 4 or video_uint8.shape[-1] != 3:
        raise ValueError(f"[cotracker_v2] video_uint8 must be [T,H,W,3], got {video_uint8.shape}")

    v = torch_mod.from_numpy(video_uint8).to(device=device)
    v = v.permute(0, 3, 1, 2).contiguous()  # [T,3,H,W]
    v = v.unsqueeze(0).float() / 255.0      # [1,T,3,H,W]
    return v


def _queries_txy_to_tyx(queries_txy: np.ndarray) -> np.ndarray:
    """
    Input contract in our pipeline: queries_txy is [Q,3] = (t, x, y)
    CoTracker commonly uses (t, y, x). We convert.
    """
    if queries_txy.ndim != 2 or queries_txy.shape[1] != 3:
        raise ValueError(f"[cotracker_v2] queries_txy must be [Q,3], got {queries_txy.shape}")
    q = queries_txy.astype(np.float32).copy()
    t = q[:, 0:1]
    x = q[:, 1:2]
    y = q[:, 2:3]
    return np.concatenate([t, y, x], axis=1)


def _safe_bool(x: np.ndarray) -> np.ndarray:
    if x.dtype == np.bool_:
        return x
    return (x.astype(np.int32) != 0)


def run_cotracker_v2(
    video_uint8: np.ndarray,
    queries_txy: np.ndarray,
    checkpoint: Optional[str] = None,
    device: Optional[str] = None,
) -> TrackerOutput:
    """
    Returns TrackerOutput with tracks in ORIGINAL video coords (x,y).

    Notes:
    - This wrapper aims to be robust across minor API differences.
    - If your cotracker install has different class/function names, adapt only the small import section below.
    """
    torch, _cuda = _require_torch()
    dev = _pick_device(torch, device)

    # --- import cotracker (robust)
    # Try common import paths; if all fail, raise with clear hints.
    model = None
    import_errs = []
    for imp in [
        "cotracker.models.build_cotracker",
        "cotracker.models.core.build_cotracker",
        "cotracker.build_cotracker",
    ]:
        try:
            mod_name, fn_name = imp.rsplit(".", 1)
            m = __import__(mod_name, fromlist=[fn_name])
            build_fn = getattr(m, fn_name)
            model = build_fn(checkpoint=checkpoint) if "checkpoint" in build_fn.__code__.co_varnames else build_fn()
            break
        except Exception as e:
            import_errs.append((imp, repr(e)))
            model = None

    if model is None:
        msg = "[cotracker_v2] Could not import/build CoTracker.\nTried:\n"
        msg += "\n".join([f"  - {p}: {e}" for p, e in import_errs[:6]])
        msg += "\n\nInstall hint (one of the following):\n"
        msg += "  - pip install cotracker\n"
        msg += "  - or install from the official CoTracker repo (editable install)\n"
        raise ImportError(msg)

    model = model.to(dev)
    model.eval()

    video_t = _to_torch_video(torch, video_uint8, dev)  # [1,T,3,H,W]
    T = int(video_uint8.shape[0])
    H = int(video_uint8.shape[1])
    W = int(video_uint8.shape[2])

    q_tyx = _queries_txy_to_tyx(np.asarray(queries_txy))
    Q = int(q_tyx.shape[0])
    if Q <= 0:
        raise ValueError("[cotracker_v2] empty queries (Q==0).")

    queries_t = torch.from_numpy(q_tyx).to(device=dev).float().unsqueeze(0)  # [1,Q,3]

    # --- forward (robust output parsing)
    with torch.no_grad():
        out = model(video_t, queries_t) if callable(model) else model.forward(video_t, queries_t)

    # out could be:
    #  - tuple(tracks, vis/occ)
    #  - dict with keys
    tracks = None
    occ = None

    if isinstance(out, (tuple, list)) and len(out) >= 1:
        tracks = out[0]
        if len(out) >= 2:
            occ = out[1]
    elif isinstance(out, dict):
        for k in ["tracks", "pred_tracks", "traj", "trajectories"]:
            if k in out:
                tracks = out[k]
                break
        for k in ["occluded", "pred_occluded", "occ", "visibility", "vis"]:
            if k in out:
                occ = out[k]
                break
    else:
        tracks = out

    if tracks is None:
        raise RuntimeError("[cotracker_v2] CoTracker output parsing failed: no tracks found.")

    # tracks common shapes:
    #  - [1,T,Q,2] or [1,Q,T,2]
    tracks_np = tracks.detach().float().cpu().numpy()
    if tracks_np.ndim != 4 or tracks_np.shape[0] != 1 or tracks_np.shape[-1] != 2:
        raise ValueError(f"[cotracker_v2] unexpected tracks shape: {tracks_np.shape}")

    # normalize to [Q,T,2]
    if tracks_np.shape[1] == T and tracks_np.shape[2] == Q:
        tracks_qt = tracks_np[0].transpose(1, 0, 2)  # [Q,T,2]
    elif tracks_np.shape[1] == Q and tracks_np.shape[2] == T:
        tracks_qt = tracks_np[0]  # [Q,T,2]
    else:
        raise ValueError(f"[cotracker_v2] cannot infer T/Q axes from tracks shape: {tracks_np.shape}, expected T={T},Q={Q}")

    # occlusion / visibility handling
    if occ is None:
        # conservative: no occlusion head -> assume all visible
        occ_qt = np.zeros((Q, T), dtype=np.bool_)
    else:
        occ_np = occ.detach().cpu().numpy() if hasattr(occ, "detach") else np.asarray(occ)
        # possible shapes: [1,T,Q] / [1,Q,T] / [T,Q] / [Q,T]
        if occ_np.ndim == 3 and occ_np.shape[0] == 1:
            occ_np = occ_np[0]
        if occ_np.shape == (T, Q):
            occ_qt = _safe_bool(occ_np.transpose(1, 0))
        elif occ_np.shape == (Q, T):
            occ_qt = _safe_bool(occ_np)
        else:
            # if it's a prob/score, squeeze and threshold
            occ_np = np.squeeze(occ_np)
            if occ_np.shape == (T, Q):
                occ_qt = (occ_np.transpose(1, 0) < 0.5) if "vis" in str(type(occ)).lower() else (occ_np.transpose(1, 0) > 0.5)
                occ_qt = _safe_bool(occ_qt)
            elif occ_np.shape == (Q, T):
                occ_qt = (occ_np < 0.5) if "vis" in str(type(occ)).lower() else (occ_np > 0.5)
                occ_qt = _safe_bool(occ_qt)
            else:
                raise ValueError(f"[cotracker_v2] unexpected occlusion/vis shape: {occ_np.shape}")

    return TrackerOutput(
        tracks_xy=tracks_qt.astype(np.float32),  # (x,y) in original coords expected by most trackers
        occluded=occ_qt.astype(np.bool_),
        video_hw=(H, W),
    )