# pr2drag/utils.py
from __future__ import annotations

import hashlib
import json
import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import yaml


def set_seed(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)


def safe_mkdir(p: str | Path) -> Path:
    p = Path(p)
    p.mkdir(parents=True, exist_ok=True)
    return p


def read_yaml(path: str | Path) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    if cfg is None:
        cfg = {}
    if not isinstance(cfg, dict):
        raise ValueError(f"YAML top-level must be a dict, got: {type(cfg)}")
    return cfg


def write_json(path: str | Path, obj: Any) -> None:
    path = Path(path)
    safe_mkdir(path.parent)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2, ensure_ascii=False)


def read_txt_lines(path: str | Path) -> List[str]:
    with open(path, "r", encoding="utf-8") as f:
        lines = [ln.strip() for ln in f.readlines()]
    return [ln for ln in lines if ln]


def sha1_of_dict(d: Dict[str, Any]) -> str:
    """
    Stable hash for cache compatibility checks.
    """
    s = json.dumps(d, sort_keys=True, separators=(",", ":"), ensure_ascii=True)
    return hashlib.sha1(s.encode("utf-8")).hexdigest()


def _is_strlike(x: Any) -> bool:
    return isinstance(x, (str, bytes, Path, np.str_, np.bytes_))


def _coerce_object_array_to_str(arr: np.ndarray, key: str) -> np.ndarray:
    """
    Convert an object-dtype ndarray that is actually "stringy" (str/bytes/Path)
    into a real string ndarray to remove dtype=object contamination.
    """
    flat = arr.ravel()
    if all(_is_strlike(x) or x is None for x in flat):
        # Convert Path/bytes/None -> str
        out_flat: List[str] = []
        for x in flat:
            if x is None:
                out_flat.append("")
            elif isinstance(x, bytes) or isinstance(x, np.bytes_):
                out_flat.append(bytes(x).decode("utf-8", errors="replace"))
            else:
                out_flat.append(str(x))
        out = np.array(out_flat, dtype=np.str_).reshape(arr.shape)
        return out

    # If it's object but not stringy, we don't silently accept it.
    # This avoids accidentally depending on pickle for arbitrary python objects.
    sample_types = sorted({type(x).__name__ for x in flat[:50]})
    raise ValueError(
        f"[npz] Key `{key}` is dtype=object but not purely str/bytes/Path. "
        f"Refuse to coerce. Sample element types: {sample_types}. "
        f"If you really expect arbitrary objects here, you must redesign the cache format."
    )


def _sanitize_arrays_for_save(arrays: Dict[str, Any]) -> Dict[str, np.ndarray]:
    """
    Enforce that saved arrays are numpy ndarrays without arbitrary python objects.
    - list[str]/list[Path] -> np.ndarray(dtype=str)
    - object arrays that are stringy -> np.ndarray(dtype=str)
    - other object arrays -> raise (prevent unsafe pickle reliance)
    """
    out: Dict[str, np.ndarray] = {}
    for k, v in arrays.items():
        if isinstance(v, np.ndarray):
            arr = v
        else:
            # np.asarray(list[Path]) becomes dtype=object; we'll fix below.
            arr = np.asarray(v)

        if arr.dtype == object:
            arr = _coerce_object_array_to_str(arr, key=k)

        out[k] = arr
    return out


def npz_write(path: str | Path, arrays: Dict[str, np.ndarray], meta: Dict[str, Any]) -> None:
    """
    Write arrays + JSON-serializable meta into a compressed NPZ.

    Design choice (robustness + security):
    - We DO NOT want to rely on pickle/object arrays in cache.
    - Therefore, before saving we coerce "stringy object arrays" into real string arrays,
      and reject other object arrays.
    """
    path = Path(path)
    safe_mkdir(path.parent)

    if meta is None:
        meta = {}
    if not isinstance(meta, dict):
        raise ValueError(f"meta must be a dict, got: {type(meta)}")

    arrays_clean = _sanitize_arrays_for_save(dict(arrays))

    meta_json = json.dumps(meta, sort_keys=True).encode("utf-8")
    payload = dict(arrays_clean)
    payload["_meta_json"] = np.frombuffer(meta_json, dtype=np.uint8)

    np.savez_compressed(path, **payload)


def npz_read(path: str | Path) -> Tuple[Dict[str, np.ndarray], Dict[str, Any]]:
    """
    Read arrays + meta from NPZ.

    Default behavior:
    1) Try allow_pickle=False (safe).
    2) If we hit the *specific* numpy error for object arrays, retry allow_pickle=True
       (only intended for *trusted* local caches generated by our own Stage1).

    Additionally:
    - If we see object arrays that are actually just strings/paths, we coerce them to dtype=str
      so downstream code never keeps dtype=object around.
    """
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"NPZ not found: {path}")

    def _read_impl(allow_pickle: bool) -> Tuple[Dict[str, np.ndarray], Dict[str, Any]]:
        arrays: Dict[str, np.ndarray] = {}
        meta: Dict[str, Any] = {}

        with np.load(path, allow_pickle=allow_pickle) as data:
            for k in data.files:
                if k == "_meta_json":
                    meta_bytes = data[k].tobytes()
                    meta = json.loads(meta_bytes.decode("utf-8"))
                    if not isinstance(meta, dict):
                        raise ValueError(f"[npz] meta must decode to dict, got: {type(meta)}")
                    continue

                v = data[k]  # may raise if object arrays and allow_pickle=False
                if isinstance(v, np.ndarray) and v.dtype == object:
                    v = _coerce_object_array_to_str(v, key=k)
                arrays[k] = v

        return arrays, meta

    try:
        return _read_impl(allow_pickle=False)
    except ValueError as e:
        msg = str(e)
        # numpy's canonical message in your stack:
        # "Object arrays cannot be loaded when allow_pickle=False"
        if "Object arrays cannot be loaded when allow_pickle=False" in msg:
            # Fallback for legacy caches (trusted).
            return _read_impl(allow_pickle=True)
        raise


def clamp(x: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, x))


def resolve_davis_root(davis_root: str | Path) -> Path:
    """
    Accept either:
      - .../DAVIS (contains JPEGImages, Annotations, ImageSets)
      - ... (parent containing DAVIS/)
    Also tolerate 'DAVIS unzipped' vs 'DAVIS_unzipped' style path drift if user passed wrong.
    """
    p = Path(davis_root)

    # Tolerate trivial drive path issues: spaces <-> underscores
    if not p.exists():
        alt = Path(str(p).replace(" ", "_"))
        if alt.exists():
            p = alt

    # If they passed parent that contains DAVIS/
    if (p / "DAVIS").exists() and not (p / "JPEGImages").exists():
        p = p / "DAVIS"

    # Validate
    need = ["JPEGImages", "Annotations", "ImageSets"]
    missing = [x for x in need if not (p / x).exists()]
    if missing:
        raise FileNotFoundError(
            f"[DAVIS] Invalid davis_root={p}. Missing subfolders: {missing}. "
            f"Expected structure: {p}/JPEGImages, {p}/Annotations, {p}/ImageSets"
        )
    return p


def davis_split_path(davis_root: Path, rel_path: str) -> Path:
    # rel_path like "ImageSets/480p/train.txt"
    rel = Path(rel_path)
    if rel.is_absolute():
        # avoid silent bugs: absolute paths override davis_root
        return rel
    return davis_root / rel


def pretty_header(title: str, kv: Dict[str, Any]) -> str:
    lines: List[str] = []
    lines.append(f"========== {title} ==========")
    for k, v in kv.items():
        lines.append(f"{k:<10}: {v}")
    lines.append("=" * (len(lines[0])))
    return "\n".join(lines)
